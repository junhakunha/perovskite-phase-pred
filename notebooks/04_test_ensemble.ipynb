{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.utils.constants import DATA_DIR, DEVICE\n",
    "from src.train import train_model\n",
    "from src.models import PhasePredictor\n",
    "from src.datasets import PhaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = np.load(os.path.join(DATA_DIR, \"labelled_dataset.npz\"))\n",
    "unlabelled_data = np.load(os.path.join(DATA_DIR, \"unlabelled_dataset.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labelled_data[\"X\"]\n",
    "Y = labelled_data[\"Y\"]\n",
    "qual_input_dims = labelled_data['X_qual_num_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junhalee/Desktop/perovskite-phase-pred/venv39/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.6822824861322131, Test Loss: 0.7322649359703064\n",
      "Epoch 30: Train Loss: 0.3456585162452289, Test Loss: 0.38584619760513306\n",
      "Epoch 60: Train Loss: 0.21739506162703037, Test Loss: 0.3940729796886444\n",
      "Epoch 90: Train Loss: 0.2282195926776954, Test Loss: 0.35482170805335045\n",
      "Epoch 120: Train Loss: 0.16089871472546033, Test Loss: 0.3575807362794876\n",
      "Epoch 150: Train Loss: 0.15791106889290468, Test Loss: 0.3570442870259285\n",
      "Epoch 180: Train Loss: 0.1498711651989392, Test Loss: 0.3572389744222164\n",
      "Epoch 210: Train Loss: 0.15520719970975602, Test Loss: 0.35730184614658356\n",
      "Epoch 240: Train Loss: 0.1876509359904698, Test Loss: 0.3573109284043312\n",
      "Epoch 270: Train Loss: 0.19752335122653417, Test Loss: 0.35732633620500565\n",
      "Epoch 300: Train Loss: 0.1872111734535013, Test Loss: 0.35733501613140106\n",
      "Epoch 330: Train Loss: 0.21292401903441974, Test Loss: 0.3573480136692524\n",
      "Epoch 360: Train Loss: 0.17260902641075, Test Loss: 0.35735470801591873\n",
      "Epoch 390: Train Loss: 0.14379279288862432, Test Loss: 0.35736532881855965\n",
      "Epoch 0: Train Loss: 0.6927142781870705, Test Loss: 0.7031501829624176\n",
      "Epoch 30: Train Loss: 0.35553039823259625, Test Loss: 0.5514456108212471\n",
      "Epoch 60: Train Loss: 0.21802797647459166, Test Loss: 0.3697018101811409\n",
      "Epoch 90: Train Loss: 0.20756264084151813, Test Loss: 0.3588294293731451\n",
      "Epoch 120: Train Loss: 0.16760867355125292, Test Loss: 0.36857545748353004\n",
      "Epoch 150: Train Loss: 0.21599774381944112, Test Loss: 0.37075623124837875\n",
      "Epoch 180: Train Loss: 0.1931439515735422, Test Loss: 0.37100188434123993\n",
      "Epoch 210: Train Loss: 0.18928834636296546, Test Loss: 0.3709254749119282\n",
      "Epoch 240: Train Loss: 0.20103417496596063, Test Loss: 0.3708901144564152\n",
      "Epoch 270: Train Loss: 0.18744851702025958, Test Loss: 0.3708561286330223\n",
      "Epoch 300: Train Loss: 0.19461011939815112, Test Loss: 0.37086116522550583\n",
      "Epoch 330: Train Loss: 0.16083215230277606, Test Loss: 0.3708336241543293\n",
      "Epoch 360: Train Loss: 0.17905902809330396, Test Loss: 0.37084394693374634\n",
      "Epoch 390: Train Loss: 0.21508606097527913, Test Loss: 0.37079155072569847\n",
      "Epoch 0: Train Loss: 0.693199634552002, Test Loss: 0.6499320864677429\n",
      "Epoch 30: Train Loss: 0.4236939400434494, Test Loss: 0.5110695585608482\n",
      "Epoch 60: Train Loss: 0.24228061150227273, Test Loss: 0.31534893438220024\n",
      "Epoch 90: Train Loss: 0.20746035714234626, Test Loss: 0.3222007341682911\n",
      "Epoch 120: Train Loss: 0.17073622505579675, Test Loss: 0.3275975324213505\n",
      "Epoch 150: Train Loss: 0.14263701252639294, Test Loss: 0.32325275242328644\n",
      "Epoch 180: Train Loss: 0.19182890414127282, Test Loss: 0.3233390972018242\n",
      "Epoch 210: Train Loss: 0.196481677038329, Test Loss: 0.3233313262462616\n",
      "Epoch 240: Train Loss: 0.17670199860419547, Test Loss: 0.32331709936261177\n",
      "Epoch 270: Train Loss: 0.20088903872030123, Test Loss: 0.32332025468349457\n",
      "Epoch 300: Train Loss: 0.1661468701703208, Test Loss: 0.323318213224411\n",
      "Epoch 330: Train Loss: 0.14997349280331815, Test Loss: 0.3233323097229004\n",
      "Epoch 360: Train Loss: 0.18942207683409965, Test Loss: 0.3233429230749607\n",
      "Epoch 390: Train Loss: 0.17860417546970503, Test Loss: 0.3233313262462616\n",
      "Epoch 0: Train Loss: 0.6913501407418933, Test Loss: 0.6721802800893784\n",
      "Epoch 30: Train Loss: 0.3393919819167682, Test Loss: 0.4841703847050667\n",
      "Epoch 60: Train Loss: 0.22668613280568803, Test Loss: 0.3686544969677925\n",
      "Epoch 90: Train Loss: 0.1722454002925328, Test Loss: 0.4058205634355545\n",
      "Epoch 120: Train Loss: 0.13809222701404775, Test Loss: 0.41100743412971497\n",
      "Epoch 150: Train Loss: 0.15146543298448836, Test Loss: 0.41380564495921135\n",
      "Epoch 180: Train Loss: 0.14524990639516286, Test Loss: 0.4134453535079956\n",
      "Epoch 210: Train Loss: 0.11645606479474477, Test Loss: 0.41350123658776283\n",
      "Epoch 240: Train Loss: 0.12664080996598517, Test Loss: 0.4134964272379875\n",
      "Epoch 270: Train Loss: 0.10617698782256671, Test Loss: 0.4134894944727421\n",
      "Epoch 300: Train Loss: 0.14205788927418844, Test Loss: 0.4134887829422951\n",
      "Epoch 330: Train Loss: 0.19073127822152205, Test Loss: 0.4134935177862644\n",
      "Epoch 360: Train Loss: 0.15623487478920392, Test Loss: 0.41350075602531433\n",
      "Epoch 390: Train Loss: 0.13652383695755685, Test Loss: 0.4135017581284046\n",
      "Epoch 0: Train Loss: 0.6863771847316197, Test Loss: 0.7067707031965256\n",
      "Epoch 30: Train Loss: 0.3115436573113714, Test Loss: 0.4967423975467682\n",
      "Epoch 60: Train Loss: 0.20947660133242607, Test Loss: 0.4066571742296219\n",
      "Epoch 90: Train Loss: 0.16688794562859194, Test Loss: 0.4165356084704399\n",
      "Epoch 120: Train Loss: 0.14234484199966704, Test Loss: 0.4243333823978901\n",
      "Epoch 150: Train Loss: 0.1642840906445469, Test Loss: 0.424197431653738\n",
      "Epoch 180: Train Loss: 0.12064066183354173, Test Loss: 0.42416127026081085\n",
      "Epoch 210: Train Loss: 0.16424259383763587, Test Loss: 0.4241723529994488\n",
      "Epoch 240: Train Loss: 0.17894170433282852, Test Loss: 0.424162358045578\n",
      "Epoch 270: Train Loss: 0.14267733373812266, Test Loss: 0.42416777089238167\n",
      "Epoch 300: Train Loss: 0.1599960054403969, Test Loss: 0.4241528548300266\n",
      "Epoch 330: Train Loss: 0.20422269191060746, Test Loss: 0.42415136843919754\n",
      "Epoch 360: Train Loss: 0.14684138979230607, Test Loss: 0.42415058985352516\n",
      "Epoch 390: Train Loss: 0.11201289083276476, Test Loss: 0.42416078597307205\n",
      "Epoch 0: Train Loss: 0.69353905745915, Test Loss: 0.6859852820634842\n",
      "Epoch 30: Train Loss: 0.3305719739624432, Test Loss: 0.37452641874551773\n",
      "Epoch 60: Train Loss: 0.18149744719266891, Test Loss: 0.35928109288215637\n",
      "Epoch 90: Train Loss: 0.17010715736874513, Test Loss: 0.3498029373586178\n",
      "Epoch 120: Train Loss: 0.20175953209400177, Test Loss: 0.34935034811496735\n",
      "Epoch 150: Train Loss: 0.18758398800023965, Test Loss: 0.34978580847382545\n",
      "Epoch 180: Train Loss: 0.2335439994931221, Test Loss: 0.34973519667983055\n",
      "Epoch 210: Train Loss: 0.19729623571038246, Test Loss: 0.3497321717441082\n",
      "Epoch 240: Train Loss: 0.21274064321603095, Test Loss: 0.34973660856485367\n",
      "Epoch 270: Train Loss: 0.21779193303414754, Test Loss: 0.34973856806755066\n",
      "Epoch 300: Train Loss: 0.16428780129977635, Test Loss: 0.3497278653085232\n",
      "Epoch 330: Train Loss: 0.23773294793707983, Test Loss: 0.3497199974954128\n",
      "Epoch 360: Train Loss: 0.19166199969393866, Test Loss: 0.34971799701452255\n",
      "Epoch 390: Train Loss: 0.2008261430476393, Test Loss: 0.3497179262340069\n",
      "Epoch 0: Train Loss: 0.6796773544379643, Test Loss: 0.6831175684928894\n",
      "Epoch 30: Train Loss: 0.37001896117414745, Test Loss: 0.4773816913366318\n",
      "Epoch 60: Train Loss: 0.2487923087818282, Test Loss: 0.42544136941432953\n",
      "Epoch 90: Train Loss: 0.23104625993541308, Test Loss: 0.42779187858104706\n",
      "Epoch 120: Train Loss: 0.1883804239332676, Test Loss: 0.4270620197057724\n",
      "Epoch 150: Train Loss: 0.20103130409760134, Test Loss: 0.4286302998661995\n",
      "Epoch 180: Train Loss: 0.1717123133795602, Test Loss: 0.4289777874946594\n",
      "Epoch 210: Train Loss: 0.21744950328554427, Test Loss: 0.42899465933442116\n",
      "Epoch 240: Train Loss: 0.2263545942093645, Test Loss: 0.42898981273174286\n",
      "Epoch 270: Train Loss: 0.20686745909707888, Test Loss: 0.4290027841925621\n",
      "Epoch 300: Train Loss: 0.22979784810117312, Test Loss: 0.42900942265987396\n",
      "Epoch 330: Train Loss: 0.2134790974003928, Test Loss: 0.4290227182209492\n",
      "Epoch 360: Train Loss: 0.17742213340742247, Test Loss: 0.4290485233068466\n",
      "Epoch 390: Train Loss: 0.19530307980520384, Test Loss: 0.4290519952774048\n",
      "Epoch 0: Train Loss: 0.6905774899891445, Test Loss: 0.7550359219312668\n",
      "Epoch 30: Train Loss: 0.3369427089180265, Test Loss: 0.5387556403875351\n",
      "Epoch 60: Train Loss: 0.22960270142980985, Test Loss: 0.33034054189920425\n",
      "Epoch 90: Train Loss: 0.2419419778244836, Test Loss: 0.33588409051299095\n",
      "Epoch 120: Train Loss: 0.1899661991213049, Test Loss: 0.33403910137712955\n",
      "Epoch 150: Train Loss: 0.23511498208556855, Test Loss: 0.3388363756239414\n",
      "Epoch 180: Train Loss: 0.12613072485796042, Test Loss: 0.3387526720762253\n",
      "Epoch 210: Train Loss: 0.27562918088265825, Test Loss: 0.3387019596993923\n",
      "Epoch 240: Train Loss: 0.23861788479345186, Test Loss: 0.3386864922940731\n",
      "Epoch 270: Train Loss: 0.22359074732022627, Test Loss: 0.3386738393455744\n",
      "Epoch 300: Train Loss: 0.2401458972266742, Test Loss: 0.3386425729840994\n",
      "Epoch 330: Train Loss: 0.19213937489049776, Test Loss: 0.3386212959885597\n",
      "Epoch 360: Train Loss: 0.21050298613096988, Test Loss: 0.3386257626116276\n",
      "Epoch 390: Train Loss: 0.1588136771959918, Test Loss: 0.338624969124794\n",
      "Epoch 0: Train Loss: 0.6943673065730503, Test Loss: 0.6911388635635376\n",
      "Epoch 30: Train Loss: 0.3572467033352171, Test Loss: 0.42208774387836456\n",
      "Epoch 60: Train Loss: 0.20796757723603929, Test Loss: 0.36375805363059044\n",
      "Epoch 90: Train Loss: 0.16259453232799256, Test Loss: 0.37299465760588646\n",
      "Epoch 120: Train Loss: 0.13004103249737195, Test Loss: 0.38124646060168743\n",
      "Epoch 150: Train Loss: 0.14127282904727118, Test Loss: 0.38210879638791084\n",
      "Epoch 180: Train Loss: 0.1204464470169374, Test Loss: 0.38236822932958603\n",
      "Epoch 210: Train Loss: 0.20322830283216067, Test Loss: 0.3823373708873987\n",
      "Epoch 240: Train Loss: 0.16191067386950767, Test Loss: 0.3823444452136755\n",
      "Epoch 270: Train Loss: 0.14189872385135718, Test Loss: 0.3823471777141094\n",
      "Epoch 300: Train Loss: 0.1763371251789587, Test Loss: 0.38234769366681576\n",
      "Epoch 330: Train Loss: 0.14149565994739532, Test Loss: 0.3823564574122429\n",
      "Epoch 360: Train Loss: 0.18319622107914516, Test Loss: 0.38235675543546677\n",
      "Epoch 390: Train Loss: 0.11227888214801039, Test Loss: 0.3823551405221224\n",
      "Epoch 0: Train Loss: 0.6910744948046548, Test Loss: 0.712030291557312\n",
      "Epoch 30: Train Loss: 0.369817924286638, Test Loss: 0.3887145668268204\n",
      "Epoch 60: Train Loss: 0.22070893592068128, Test Loss: 0.31919578090310097\n",
      "Epoch 90: Train Loss: 0.14326760598591395, Test Loss: 0.33655536361038685\n",
      "Epoch 120: Train Loss: 0.1634101245020117, Test Loss: 0.33707356452941895\n",
      "Epoch 150: Train Loss: 0.16138246256325925, Test Loss: 0.3374154958873987\n",
      "Epoch 180: Train Loss: 0.17360318984304154, Test Loss: 0.3375511895865202\n",
      "Epoch 210: Train Loss: 0.1411505602300167, Test Loss: 0.3376095946878195\n",
      "Epoch 240: Train Loss: 0.15568751575691359, Test Loss: 0.33761272579431534\n",
      "Epoch 270: Train Loss: 0.16547302715480328, Test Loss: 0.33761334978044033\n",
      "Epoch 300: Train Loss: 0.1142621035022395, Test Loss: 0.33762614242732525\n",
      "Epoch 330: Train Loss: 0.17682285553642682, Test Loss: 0.33763142116367817\n",
      "Epoch 360: Train Loss: 0.16645729076117277, Test Loss: 0.33764003962278366\n",
      "Epoch 390: Train Loss: 0.19285720214247704, Test Loss: 0.3376459088176489\n"
     ]
    }
   ],
   "source": [
    "num_models = 10\n",
    "\n",
    "models = [PhasePredictor(input_dim=X.shape[1], qual_input_dims=qual_input_dims, latent_dimension=64) for i in range(num_models)]\n",
    "for model in models:\n",
    "    model.to(device)\n",
    "    train_model(model, X, Y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(X):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model(X)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Get the mean and variance of the predictions\n",
    "    predictions = torch.stack(predictions)\n",
    "    mean = predictions.mean(dim=0)\n",
    "    var = predictions.var(dim=0)\n",
    "\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1664, 37)\n"
     ]
    }
   ],
   "source": [
    "print(unlabelled_data['X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(unlabelled_data['X'], device=device, requires_grad=False).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, var = inference(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9969, grad_fn=<MaxBackward1>) tensor(0.0765, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(mean.max(), var.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
